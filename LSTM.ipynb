{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a41be5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt \n",
    "from IPython.display import Image\n",
    "\n",
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0abbd281",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "\n",
    "def split(word):\n",
    "    return [char for char in word]\n",
    "    \n",
    "def padding(word):\n",
    "    while len(word)<100:\n",
    "        word = word + '\\0'\n",
    "    return word\n",
    "    \n",
    "def one_hot_encoding(data):\n",
    "    mapped={'a':0,'b':1,'c':2,'\\0':3}\n",
    "    onehot_encoded = list()\n",
    "    for element in data:\n",
    "        letter = [0,0,0,0]\n",
    "        letter[mapped[element]] = 1\n",
    "        onehot_encoded.append(letter)\n",
    "    return onehot_encoded\n",
    "    \n",
    "\n",
    "def hot_encode(padded_words):\n",
    "    encoded_words=[]\n",
    "    for word in padded_words:\n",
    "        encoded_words.append(torch.tensor(one_hot_encoding(word)))\n",
    "    return encoded_words    \n",
    "        \n",
    "def count(words):\n",
    "    counts=[]\n",
    "    for e in words:\n",
    "        counts.append(torch.tensor(list(Counter(e).values())))\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3138308",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    def __init__(self):\n",
    "        self.data = 'cfg_dataset.csv'\n",
    "        self.max_len = 100\n",
    "        self.tokens=[]\n",
    "        self.words=[]\n",
    "        self.padded=[]\n",
    "        self.labels=[]\n",
    "        self.sequ=[]\n",
    "        \n",
    "        self.load_data()\n",
    "        self.add_padding()\n",
    "        self.tokenize()\n",
    "        self.token_to_seq()\n",
    "        \n",
    "    def load_data(self):\n",
    "        df = pd.read_csv('cfg_dataset.csv')\n",
    "        self.labels =df['Class'].values\n",
    "        self.words = df['Words'].values\n",
    "        \n",
    "    def tokenize(self): #splitting words into lists of characters\n",
    "        for e in self.padded:\n",
    "            self.tokens.append(split(e))\n",
    " \n",
    "    def display(self):\n",
    "        for e in self.words[:20]:\n",
    "            print(e,'\\n' )\n",
    "\n",
    "#     def token_to_seq(self):   # replacing each token with an integer representation\n",
    "#         for i in range(len(self.tokens)):\n",
    "#             tmp=[]\n",
    "#             for e in self.tokens[i]:\n",
    "#                 if e =='a' :\n",
    "#                     tmp.append(25)\n",
    "#                 elif e=='b':\n",
    "#                     tmp.append(50)\n",
    "#                 elif e=='c':\n",
    "#                     tmp.append(75)\n",
    "#                 else:\n",
    "#                     tmp.append(0)\n",
    "#             self.sequ.append(tmp)\n",
    "\n",
    "\n",
    "\n",
    "    def token_to_seq(self):   #replacing each token with it's ASCII code\n",
    "        for e in self.tokens:\n",
    "            self.sequ.append([ord(a) for a in e])\n",
    "    \n",
    "    def add_padding(self):  #Adds padding to each word to reach a length of 100\n",
    "        for e in self.words:\n",
    "            self.padded.append(padding(e))\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d14d80fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):  #Creating a dataset class inheriting from torch Dataset which will allow the use of the torch dataloader\n",
    "    def __init__(self,data):\n",
    "        self.data=data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        word_seq,class_nbr = self.data[idx]\n",
    "#         class_nbr=torch.tensor(class_nbr)\n",
    "        word_tensor=torch.tensor(word_seq)\n",
    "        return word_tensor, class_nbr\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Custom dataset containing cfg words and their classes \\n'+ 'Contains '+ str(len(self.data))+ ' words'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e73b1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=Preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbc08587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaabcccccccccccccccccc \n",
      "\n",
      "aabbbccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc \n",
      "\n",
      "aaaaaaaaaaaaaabcccccccccccccccccccccccccccc \n",
      "\n",
      "aaabcccccccccccccccccccccccccccccccccccccccccc \n",
      "\n",
      "aaaaaaaaaaaaaaaaaaaaaaabbbbbbcccccccccccccccccccccccccccccccccccccccccccccc \n",
      "\n",
      "aaaaaabbccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc \n",
      "\n",
      "abccccccccccccccc \n",
      "\n",
      "aaabbcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc \n",
      "\n",
      "aabbbbbbbccccccccccccccccccccccccccccccccccccccccccc \n",
      "\n",
      "abcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc \n",
      "\n",
      "aaaaaaaaaaaaaaaaaaabbbcccccccccccccccccccccccccccccccccccccc \n",
      "\n",
      "abccccccccccccccccccccc \n",
      "\n",
      "aaaabbbbccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc \n",
      "\n",
      "aabbbbbbbbbbbcccc \n",
      "\n",
      "abbbbbbbbbbbbbbbbbbbbbbbbbbcc \n",
      "\n",
      "aabbccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc \n",
      "\n",
      "abbcccccccccccccccccccccccccccccccccc \n",
      "\n",
      "aaaaaaaaaaabbbcccccccccccccccccccccc \n",
      "\n",
      "aabbbbbcccc \n",
      "\n",
      "aaabbccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc \n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.display() # Displays first 20 elements onf the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39b3f140",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_temp=list(zip(data.sequ,data.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0500e13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom dataset containing cfg words and their classes \n",
      "Contains 700 words\n"
     ]
    }
   ],
   "source": [
    "dataset = MyDataset(data_temp)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f85888f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([97, 97, 97, 98, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99,\n",
       "         99, 99, 99, 99,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56e07472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([97, 97, 97, 98, 98, 98, 98, 98, 98, 99, 99, 99, 99, 99, 99,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4861b89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = random_split(dataset,[550, 150])\n",
    "batch_size=32\n",
    "train_loader=DataLoader(train_dataset, batch_size, shuffle= True)\n",
    "test_loader= DataLoader(test_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fb6a669",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = 128\n",
    "        self.LSTM_Layers = 2\n",
    "        self.input_size = 100\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.input_size, self.hidden_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=self.hidden_dim, hidden_size=self.hidden_dim, num_layers=self.LSTM_Layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(in_features=self.hidden_dim, out_features=self.hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        h = torch.zeros((self.LSTM_Layers, x.size(0), self.hidden_dim))\n",
    "        c = torch.zeros((self.LSTM_Layers, x.size(0), self.hidden_dim))\n",
    "        \n",
    "        torch.nn.init.xavier_normal_(h)\n",
    "        torch.nn.init.xavier_normal_(c)\n",
    "\n",
    "        out = self.embedding(x)\n",
    "        out, (hidden, cell) = self.lstm(out, (h,c))\n",
    "        out = self.dropout(out)\n",
    "        out = torch.relu_(self.fc1(out[:,-1,:]))\n",
    "        out = self.dropout(out)\n",
    "        out = torch.sigmoid(self.fc2(out))\n",
    "\n",
    "        return out  \n",
    "    \n",
    "    def infer(self,batch):\n",
    "        seq,label = batch\n",
    "        seq = seq.type(torch.LongTensor)\n",
    "        label = label.type(torch.FloatTensor)\n",
    "        label= label.unsqueeze(1)\n",
    "        out = self.forward(seq)\n",
    "        loss=F.binary_cross_entropy(out,label)\n",
    "        acc = accuracy(label,out)           \n",
    "        return loss,acc\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1674d2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def accuracy(grand_truth, predictions):\n",
    "        true_positives = 0\n",
    "        true_negatives = 0\n",
    "        for true, pred in zip(grand_truth, predictions):\n",
    "            if (pred > 0.5) and (true == 1):\n",
    "                true_positives += 1\n",
    "            elif (pred < 0.5) and (true == 0):\n",
    "                true_negatives += 1\n",
    "            else:\n",
    "                pass\n",
    "        return (true_positives+true_negatives) / len(grand_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7f4e7a",
   "metadata": {},
   "source": [
    "### Training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42179e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, lr, model, train_loader,batch):      \n",
    "    optimizer=optim.RMSprop(model.parameters(), lr)  \n",
    "    train_History=[]\n",
    "    for epoch in range(epochs):\n",
    "        batch_train_losses=[]\n",
    "        batch_train_accs=[] \n",
    "        for batch in train_loader:        \n",
    "            loss,acc=model.infer(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            batch_train_losses.append(loss)\n",
    "            batch_train_accs.append(acc)\n",
    "        epoch_train_acc = sum((batch_train_accs)) / float(len((batch_train_accs)))\n",
    "        epoch_train_loss = torch.stack(batch_train_losses).mean()   \n",
    "        train_History.append([epoch_train_loss,epoch_train_acc])\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, train_acc: {:.4f}\".format(epoch, epoch_train_loss, epoch_train_acc))\n",
    "    return train_History\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5337d041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    test_History=[]\n",
    "    batch_test_losses=[]\n",
    "    batch_test_accs=[] \n",
    "    for batch in test_loader:        \n",
    "        loss,acc=model.infer(batch)\n",
    "        batch_test_losses.append(loss)\n",
    "        batch_test_accs.append(acc)\n",
    "    test_acc = sum((batch_test_accs)) / float(len((batch_test_accs)))\n",
    "    test_loss = torch.stack(batch_test_losses).mean()\n",
    "    print(\" test_loss: {:.4f}, test_acc: {:.4f}\".format( test_loss, test_acc))\n",
    "    return test_acc,test_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "935e710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb20ab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LSTMClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995cfa50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 0.6645, train_acc: 0.6047\n",
      "Epoch [1], train_loss: 0.5408, train_acc: 0.7170\n",
      "Epoch [2], train_loss: 0.4442, train_acc: 0.8183\n",
      "Epoch [3], train_loss: 0.3743, train_acc: 0.8414\n",
      "Epoch [4], train_loss: 0.3661, train_acc: 0.8524\n",
      "Epoch [5], train_loss: 0.3116, train_acc: 0.8854\n"
     ]
    }
   ],
   "source": [
    "History=fit(15,0.001,model, train_loader,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d766aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss =[torch.detach(f_elemnt[0]).numpy() for f_elemnt in History]\n",
    "Acc = [s_elemnt[1] for s_elemnt in History]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2bda9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.linspace(1, 15, 15).astype(int)\n",
    "xnew = np.linspace(X.min(), X.max(),150) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e49b37d",
   "metadata": {},
   "source": [
    "### Training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4075857a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=Loss\n",
    "spl = make_interp_spline(X, Y, k=3)  # type: BSpline\n",
    "power_smooth = spl(xnew)\n",
    "plt.plot(xnew, power_smooth,label='Loss')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e90622b",
   "metadata": {},
   "source": [
    "### Training Acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59170188",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=Acc\n",
    "spl = make_interp_spline(X, Y, k=3)  # type: BSpline\n",
    "power_smooth = spl(xnew)\n",
    "plt.plot(xnew, power_smooth,label='Acc')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a80037",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics= test(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
